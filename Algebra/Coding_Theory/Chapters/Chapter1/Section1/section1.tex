%----------------------------------------------------------------------------------------
%	SECTION 1.1
%----------------------------------------------------------------------------------------

\section{Probability Theory.}
\label{section1}

\begin{definition}
    We define a \textbf{probability space} to be a triple $(S,\Bc, P)$ where
    $S$ is a nonempty set called the  \textbf{sample space}, $\Bc$ is a Borel
    set of subsets of  $S$, and  $P$ is a nonnegative function on  $\Bc$ with
    the properties
    \begin{enumerate}
        \item[(1)] $P(S)=1$.

        \item[(2)] $P(\bigcup_{n=1}^{\infty}{A_n})=\sum_{n=1}^{\infty}{P(A_n)}$.
    \end{enumerate}
    where each $A_m \cap  A_n=\emtyset$ whenever  $m \neq n$. We call  $P$ the
     \textbf{probability measure}. We call the value of $P(A)$ the
     \textbf{probaility} of $A$.
\end{definition}

\begin{example}
    If $S=\{s_1, s_2, \dots\}$ is finite, or countable, and $\Bc=2^S$, and
    $\{p_n\}$ is a sequence of nonegative numbers with $\sum{p_n}=1$, then the
    function defined by $P(A)=\sum{\{p_n : s_n \in A\}}$ is a probability
    measure, and makes $(S,\Bc, P)$ a prpbability space.
\end{example}

\begin{definition}
    Let $S$ be the sample space of some probability space with measure  $P$. We
    define a \textbf{random variable} to be a mapping $X:S \rightarrow R$, where
    we call $R$ the  \textbf{range} of $X$. If  $R$ is a subset of an
    $n$-dimensional Euclidean space, then we call $X$ a  \textbf{random vector},
    whose components are random variables with $1$-dimensional range.
\end{definition}

\begin{definition}
    Let $(S, \Bc, P)$ be a probability space. We define two random variables $X$
    and  $Y$ to be  \textbf{equal almost everywhere} if the probability of the
    set $\{s: X(s) \neq Y(s)\}=0$, i.e.  $P(X) \neq P(Y)$. We write $X=Y$.
\end{definition}

\begin{definition}
    We define the \textbf{expectation}, or  \textbf{average} of a random
    variable $X$, with range  $\R$ to be the function $E(X)$, defined by:
    \begin{equation}
        E(X)=\int_{S}{X(s)} \ \dd{P}
    \end{equation}
    where the integral take is the Lebesgue integral.
\end{definition}

\begin{definition}
    Let $S$ be a sample space with probability measure  $P$. We define the
    \textbf{distribution} of a random variable $X$ with range $R$ to be the
    probability measure $P_X$ induced by  $P$ on  $R$ such that  $P_X(A)=\{s :
    X(s) \in A\}$, for any Borel set of $\R$. We define the function:
    \begin{equation}
        F_X(x)=P(\{s : X(s \leq x)\})
    \end{equation}
    to be the \textbf{distribution} function of $X$.
\end{definition}

We can see that the expectation of $X$ via the above definitions is given by:
\begin{equation*}
    E(X)=\int_{R}{x} \ \dd{P_x}=\int_{-\infty}^{\infty}{x} \ \dd{F_X(x)}
\end{equation*}

Where the integral taken with respect to $P_X$ is Lebeusue, and the integral
taken with respect to  $F_X(x)$ is Riemann-Stieltjes.

In the case where $S$ is discrete, and hence so is  $R$, for any random vairable
$X$, we can then define the expectations of  $X$ to be the sums:
    \begin{align}
        E(X)        &=      \sum_{x}{p(x)x} \\
        E(f(X))     &=      \sum_{x}{p(x)f(x)}
    \end{align}
where $f:S \rightarrow \R$ and $f(X)$ is a random variable. It may be that $f$
is not well behaved, or undefined for certain values of  $x$.

\begin{definition}
    Let $X$ be a random variable. We say that $X$ has  \textbf{density} if the
    distribution function of $X$ has the form:
    \begin{equation}
        F_X(x)=\int_{-\infty}^x{up(u)} \ \dd{u}
    \end{equation}
    for some nonegative function $p$. We then define the expectations of  $X$ to
    be
    \begin{align}
        E(X)        &=      \int_{-\infty}^{\infty}{up(u)} \ \dd{u} \\
        E(f(X))        &=      \int_{-\infty}^{\infty}{f(u)p(u)} \ \dd{u} \\
    \end{align}
\end{definition}

\begin{definition}
    Let $(S,\Bc, P)$ be a probability space, and let $A_1, \dots, A_n \in \Bc$.
    We say that $A_1, \dots, a_n$ are \textbf{independent} if for every subset
    of them, $A_{i_1}, \dots, A_{i_m}$,
    \begin{equation}
        P(A_{i_1} \cap \dots P(A_{i_m}))=P(A_{i_1}) \dots P(A_{i_m})
    \end{equation}
    Moreover, a collection $X_1, \dots X_n$ of random variables are indepenedent
    if the evets $A_i=\{s : X_i(s) \in S_i\}$ are indepenedent for any $S_1,
    \dots S_n \in \Bc$.
\end{definition}

\begin{definition}
    If $X$ and  $Y$ are real random variables defined on a sample space  $S$,
    then the mapping  $s \rightarrow (X(s), Y(s))$ induces a probability measure
    $P_{XY}$ on the field of $2$-dimensional Borel sets, called the
    \textbf{joint distribution} which is defined by the function
    \begin{equation}
        F_{XY}(x,y)=P(\{x : X(s) \leq x, Y(s) \leq y\})
    \end{equation}
\end{definition}

\begin{lemma}\label{1.1.1}
    The random variables $X$ and  $Y$ are independent if, and only if
    $F_{XY}=F_X(x)F_Y(y)$. With $F_X$ and  $F_Y$ the distributions of  $X$ and
    $Y$, respectively.
\end{lemma}

\begin{theorem}[The Weak Law of Large Numbers.]\label{1.1.2}
    Let $X_1, \dots X_n$ be independent random variables, each with finite
    expectation $\mu$, and with the same distribution. Then for each
    $\epsilon>0$,
    \begin{equation}
        \lim_{n \rightarrow \infty}{P(|\frac{\sum{X_i}}{n}-\mu| \geq \epsilon)=0}
    \end{equation}
\end{theorem}
