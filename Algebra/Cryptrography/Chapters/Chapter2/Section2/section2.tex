%----------------------------------------------------------------------------------------
%	SECTION 1.1
%----------------------------------------------------------------------------------------

\section{Entropy}
\label{section1}

\begin{definition}
    Let $X$ be a discrete random variable. We define the  \textbf{entropy} of
    $X$ to be:
    \begin{equation}
        H(X)=-\sum_{x}{P(x)\log{P(x)}}
    \end{equation}
    wher $\log$ is the logarithm base  $2$. When  $P(x)=0$, we define
    $P(x)\log{P(x)}=0$.
\end{definition}

\begin{example}
    \begin{enumerate}
        \item[(1)] Let $X$ be a random variable with sample size  $n$. If
            $P(X=x)=\frac{1}{n}$, then $H(X)=\log{n}$.

        \item[(2)] Let $(\Pc, \Cc, \Kc)$ be the cryptosystem defined in example
            $(2.1)$, then
            $H(\Pc)=-\frac{1}{4}\log{\frac{1}{4}}-\frac{3}{4}\log{\frac{3}{4}}=
            2-\frac{3}{4}\log{3}$. So $\Pc$ gives about  $0.81$ bits of
            uncertainty. Similarly,  $H(\Cc)=1.85$ and $H(\Kc)=1.5$.
    \end{enumerate}
\end{example}

\begin{definition}
    We define a real-valued function $f$ to be  \textbf{concave} on an interval
    $I$ if
    \begin{equation}
        f(\frac{x+y}{2}) \geq \frac{f(x)+f(y)}{2}
    \end{equation}
    We say $f$ is  \textbf{strictly concave} on $I$ if
    \begin{equation}
        f(\frac{x+y}{2}) > \frac{f(x)+f(y)}{2}
    \end{equation}
    for all  $x,y \in I$.
\end{definition}

\begin{theorem}[Jensen's Inequality]\label{2.2.1}
    Let $f$ be a continuous real-valued function on an interval $I$, and
    suppose  $\sum{a_i}=1$ for some sequence $\{a_i\}_{i=1}^n$ where $a_i>0$ for
    all  $i$. Then:
    \begin{equation}
        \sum{f(a_ix_i)} \leq f(\sum{a_ix_i})
    \end{equation}
    given a sequence $\{x_i\}_{i=1}^n \subseteq I$.
\end{theorem}
\begin{corollary}
    Equality holds when $x_1=\dots=x_n$.
\end{corollary}

\begin{theorem}\label{2.2.2}
    For any random variable $X$ with probaility distribution  $\{p_i\}_{i=1}^n$,
    we have $0 \leq H(X) \leq \log{n}$.
\end{theorem}
\begin{proof}
    First notice that since $\{p_i\}$ is a probability distribution,  $p_i>0$
    for all  $i$ and  $\sum{p_i}=1$. Now, that $0 \leq H(X)$ follows from
    definition. Then, by Jensen's inequality,
    $H(X)=-\sum{p_i\log{p_i}}=\sum{p_i\log{\inv{p_i}}} \leq
    \log{\sum{p_i\inv{p_i}}}=\log{n}$.
\end{proof}
\begin{corollary}
    $H(X)=0$ when atleast one $p_i=0$, and  $H(X)=\log{n}$ if $p_i=\frac{1}{n}$
    for all $i$.
\end{corollary}

\begin{theorem}\label{2.2.3}
    $H(X,Y) \leq H(X)+H(Y)$, with equality if, and only if $X$ and  $Y$ are
    independent random variables.
\end{theorem}

\begin{definition}
    Let $X$ and $Y$ be random variables. We define the \textbf{conditional
    entropy} of $X$ given  $Y=y$, and  $X$ given $Y$ to be:
    \begin{equation}
        H(X|y)=-\sum_x{P(x|y)\log{P(x|y)}} \\
    \end{equation}
    and
    \begin{equation}
        H(X|Y)=\sum_{y}{P(y)H(X|y)}=-\sum_y{\sum_x{P(y)}P(x|y)\log{P(x|y)}} \\
    \end{equation}
\end{definition}

\begin{theorem}\label{2.2.4}
    $H(X,Y)=H(Y)+H(X|Y)$
\end{theorem}
\begin{corollary}
    $H(X|Y) \leq H(X)$ with equality if, and only if $X$ and  $Y$ are
    independent.
\end{corollary}
