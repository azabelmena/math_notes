%----------------------------------------------------------------------------------------
%	SECTION 1.1
%----------------------------------------------------------------------------------------

\section{Invertible Matrices.}
\label{section1}

\begin{definition}
    Let $A$ be an  $n \times n$ square matrix over a field  $F$. We call an $n \times n$ matrix  $B$
    a  \textbf{left inverse} of $A$ if  $BA=I$, we call  $B$ a \textbf{right inverse} if $AB=I$. We    
    call  $B$ the  \textbf{inverse} of $A$ if it is both a left and right inverse, and we say $A$ is
    invertible. We denote the inverse of  $A$ as  $A^{-1}$.
\end{definition}

We can now state a fundamental property of all $n \times n$ matrices.

\begin{lemma}\label{1.5.1}
    If $A$ has a left inverse  $B$, and a right inverse  $C$, then  $B=C$ and  $A$ is invertible.
\end{lemma}
\begin{proof}
    We have $BA=I$ and  $AC=I$. Then  $B=BI=B(AC)=(BA)C=IC=C$.
\end{proof}

\begin{theorem}\label{1.5.2}
    Let $\ast$ be the matrix product. Then  $F^{n \times n}$ forms a group over $\ast$.
\end{theorem}
\begin{corollary}
    Let $A$ and  $B$ be  $n \times n$ matrices. Then:
    \begin{enumerate}
        \item[(1)] If $A$ is invertible, then so is  $A^{-1}$, and $(A^{-1})^{-1}=A$.		

        \item[(2)] If $A$ and $B$ are invertible, then so is $AB$, and
            $(AB)^{-1}=B^{-1}A^{-1}$.
    \end{enumerate}		
\end{corollary}
\begin{proof}
    This follows from group theory.
\end{proof}

\begin{corollary}
    Products of invertible matrices are invertible.
\end{corollary}

\begin{theorem}\label{1.5.3}
    Elementary matrices are invertible.
\end{theorem}
\begin{proof}
    Let $e$ be an elementary row operation. Let  $E=e(I)$ and $E'=e^{-1}(I)$. Then
    $EE'=e(e^{-1}(I))=I$ and $E'E=e^{-1}(e(I))=I$; so $E$ is invertible and  $E'=E^{-1}$.
\end{proof}

\begin{example}
    \begin{enumerate}
        \item[(1)] $\begin{pmatrix}
                    0 & 1 \\
                    1 & 0 \\
            \end{pmatrix}^{-1}=
            \begin{pmatrix}
                    0 & 1 \\
                    1 & 0 \\
            \end{pmatrix}$


        \item[(2)] $\begin{pmatrix}
                    0 & c \\
                    1 & 0 \\
            \end{pmatrix}^{-1}=
            \begin{pmatrix}
                    0 & -c \\
                    1 & 0 \\
            \end{pmatrix}$

        \item[(3)] $\begin{pmatrix}
                    0 & 1 \\
                    c & 0 \\
            \end{pmatrix}^{-1}=
            \begin{pmatrix}
                     0 & -c \\
                    -c & 0 \\
            \end{pmatrix}$

        \item[(4)] For $c \neq 0$, $\begin{pmatrix}
                    c & 0 \\
                    0 & 1 \\
            \end{pmatrix}^{-1}=
            \begin{pmatrix}
                     c^{-1} & 0 \\
                         0 & 1 \\
            \end{pmatrix}$ and
            $\begin{pmatrix}
                    1 & 0 \\
                    0 & c \\
            \end{pmatrix}^{-1}=
            \begin{pmatrix}
                     1 & 0 \\
                     0 & c^{-1} \\
            \end{pmatrix}$
    \end{enumerate}		
\end{example} 

\begin{theorem}\label{1.5.4}
    If $A$ is an  $n \times n$ matrix, the following are equivalent:
        \begin{enumerate}
            \item[(1)] $A$ is invertible.

            \item[(2)] $A$ is row equivalent to  $I$.

            \item[(3)] $A$ is a product of elementary row matrices.
        \end{enumerate}
\end{theorem}
\begin{proof}
    Suppose that $A$ is invertible. Let $R$ be a row reduced echelon matrix, row equivalent to  
    $A$. Then  $R=E_k \dots E_2E_1A$. By the corolloray to theorem \ref{1.5.3}, we have $E_i$ is 
    invertible for each $1 \leq i \leq k$. Then  $A=E_1^{-1}E_2^{-1} \dots E_k^{-1}R$. So  $A$ 
    is invertible if and only if  $R$ is invertible. Since $R$ is square, it is invertible if 
    and only if  $R=I$. Thus by hypothesis, $A=E_1^{-1}E_2^{-1} \dots E_k^{-1}$. This also implies
    that $A$ is row equivalent to  $I$.

    Now if  $A$ is row equivalent to  $I$, then  $I=PA$ where  $P=E_k \dots E_1$. We have then that
    $A=AI=A(PA)=(AP)A$, which makes $AP=I$, hence  $A$ is invertible.
\end{proof}
\begin{corollary}
    If $A$ is invertible, the sequence of elementary matrices  $\{E_i\}_{i=1}^k$ taking $A
    \rightarrow I$ has product $A^{-1}$; that is $A^{-1}=E_k \dots E_1$.
\end{corollary}
\begin{corollary}
    If $A$ and  $B$ are  $m \times n$ matrices,  $B$ is row equivalent to  $A$ if, and only if
    $B=PA$, where $P$ is an  $m \times m$ invertible matrix.
\end{corollary}

\begin{theorem}\label{1.5.5}
    If $A$ is an  $n \times n$ matrix, the following are equivalent:
        \begin{enumerate}
            \item[(1)] $A$ is invertible.

            \item[(2)] The system $AX=0$ has only the trivial solution.

            \item [(3)] The system $AX=Y$ has a solution  $X$ for each  $m \times 1$ matrix  $Y$.
        \end{enumerate}
\end{theorem}
\begin{proof}
    That (1) implies (2) implies  (3) is clear and follows from theorems \ref{1.3.4} and
    \ref{1.5.4}. Now suppose that $AX=Y$ has a solution for each  $Y$:
        \begin{equation*}
            E= \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \\ \end{pmatrix}
        \end{equation*}
    If $RX=E$ can be solved for  $X$, the least row is not  $0$. Now  $R=PA$ with  $P$ invertible,
    thus  $RX=E$ if and only if  $AX=P^{-1}E$, which has a solution.
\end{proof}
\begin{corollary}
    A square matrix with either left or right (or both) inverses is invertible.
\end{corollary}
\begin{proof}
    If $A$ has both left and right inverses, we are done. Suppose then that  $A$ is square with left
    inverse  $B$, then  $BA=I$. Now  $AX=0$ has only the trivial soltion, since  $X=IX=(BA)X=B(AX)$,
    making $A$ invertible. The same holds for right inverses by similar reasoning.
\end{proof}
\begin{corollary}
    Let $A=A_1A_2 \dots A_k$, where $A_i$ is an  $n \time n$ matrix for each  $1 \leq i \leq k$.
    Then  $A$ is invertible if and only if each  $A_i$ is invertible.
\end{corollary}
\begin{proof}
    If $A_i$ is invertible for each  $1 \leq i \leq k$, then so are the products, so  $A$ is also
    invertible.

    Now suppose that  $A$ is invertible. suppose that  $X$ is an  $n \times 1$ matrix and that
    $A_kX=0$. Then  $AX=(A_1A_2 \dots A_{k-1})A_kX=0$, implying that $A_kX=0$ has only the trivial
    solution. So $A_k$ is invertible, hence  $A_1 \dots A_{k-1}=AA_k^{-1}$ is invertible. Extending
    this reasoning to $A_i$ for $1 \leq i \leq k-1$, we get $A_i$ invertible.
\end{proof}

\begin{example}
    Let $F=\Q$ and consider  $A=\begin{pmatrix} 2 & -1 \\ 1 & 3 \\ \end{pmatrix}$. Consider $AX=Y$,
    then:
        \begin{align*}
            \begin{pmatrix}
                2 & -1 & y_1 \\
                1 & 3 & y_2 \\
            \end{pmatrix} \rightarrow_3
            \begin{pmatrix}
                1 & 3 & y_2 \\
                2 & -1 & y_1 \\
            \end{pmatrix} \rightarrow_2
            \begin{pmatrix}
                1 & 3 & y_2 \\
                0 & -7 & y_1-2y_2 \\
            \end{pmatrix} \rightarrow_1 \\
            \begin{pmatrix}
                1 & 3 & y_2 \\
                0 & 1 & \frac{1}{7}(y_1-2y_2) \\
            \end{pmatrix} \rightarrow_2
            \begin{pmatrix}
                1 & 0 & \frac{1}{7}(y_2+3y_1) \\
                0 & 1 & \frac{1}{7}(y_1-2y_2) \\
            \end{pmatrix}
        \end{align*}
    We get that $A$ is invertible and that $A=\begin{pmatrix} \frac{3}{7} & \frac{1}{7} 
    \\ -\frac{1}{7} & \frac{2}{7} \\ \end{pmatrix}$. 
\end{example}

This example showds us that we can find the inverse of any invertible matrix $A$ by forming the
system  $AX=Y$, and taking  $(A|Y) \rightarrow (I|Y')$, where $Y'=A^{-1}Y$. Even better, we can
compute $A^{-1}$ by taking $(A|I) \rightarrow (I|A^{-1})$.
