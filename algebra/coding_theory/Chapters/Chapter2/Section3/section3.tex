%----------------------------------------------------------------------------------------
%	SECTION 1.1
%----------------------------------------------------------------------------------------

\section{The Hamming Metric.}
\label{section1}

\begin{definition}
    Let $F$ be a field. The  \textbf{Hamming distance} of two vectors $x,y \in
    F^n$ is the number of components for which  $x_i \neq y_i$. That is,
    $d_H(x,y)=w_H(y-x)$.
\end{definition}

\begin{lemma}\label{2.3.1}
    Hamming distance makes $F^n$ into a metric space.
\end{lemma}

\begin{figure}[h]
    \centering
    \includegraphics[scale=1.0]{Figures/Chapter2/hamming_spheres.eps}
    \caption{Hamming spheres around adjacent codewords \cite{mceliece}.}
    \label{fig_2.1}
\end{figure}

\begin{definition}
    Let $\Cc$ be an $(n,k)$-code. We define the \textbf{Hamming sphere} of
    radius $r$ about a codeword  $x \in \Cc$ to be the open ball  $B_r(x)=\{y
    \in \Cc : d_H(x,y)<r\}$, where $d_H$ is the Hamming distance of  $x$ and
    $y$.
\end{definition}

\begin{definition}
    Let $\Cc$ be a code  (not necessarily linear). The \textbf{minumum distance}
    of $\Cc$ is the smalles Hamming distance across all codewords; i.e.
    \begin{equation}
        d=\min{\{d_H(x,y) : x,y \in \Cc \text{ and } x \neq y\}}
    \end{equation}
    Similarly, we define the \textbf{minimum weight} of $\Cc$ to be:
    \begin{equation}
        w=\min{\{w_H(x) : x \in \Cc \text{ and } x \neq 0\}}
    \end{equation}
\end{definition}

\begin{theorem}\label{2.3.2}
    A code $\Cc$ with minumum distance $d$ can correct $e$ errors across all
    error patterns if, and only if $d \geq 2e+1$
\end{theorem}
\begin{proof}
    Let $\Cc$ be a code of length  $n$  (not necessarily linear), and suppose
    the codeword $x_i$ is sent through a  $q$SC as $y=x_i+z$, with  $w_H(z)=e$.
    Then if each codeword is sent through the channel with probability
    $\frac{1}{M}$, pick the codeword $y$ such that  $d_H(x_i,y)$ is the smallest
    possible. Notice that $d_H(x,y)=w_H(z)$.

    Now, for $x_j \in \Cc$ such that, $x_j \neq x_j$ if $d_H(x_i,y) \geq 2e+1$,
    then the hammng spheres $B_e(x_i)$ and $B_e(x_j)$ are disjoint; see
    figure \ref{fig_2.1}. For if not, and there is a $y \in B_e(x_i) \cap
    B_e(x_j)$, then $d_H(x_i,x_j) \leq d_H(x_i,y)+d_H(y,x_j)<e+e=2e$, which
    contradicts the assumption. Thus, if $d_H(x,y) \leq e$, then $y \in
    B_e(x_i)$. This means, if the difference between the components of $x_i$ and
    $y$ is less than $e$, $y$ cannot be closer to  $x_j$ than it is to  $y$, and
    so we can choose to decode it as  $x_i$.

    On the other hand, if  $d_H(x_i,x_j) \leq 2e$, then we have that $B_e(x_i)
    \cap B_e(x_j) \neq \emptyset$. Then if $d_H(x_i,y) \leq e$, then $y \in
    B_(x_i) \cap B_e(x_j)$, and so $y$ is as close to $x_i$ as it is to $x_j$,
    so we cannot make a reasonable choice as how to decode  $y$.

    Therefore we have that the code  $\Cc$ can correct  $e$ errors if, and only
    if the distance between any two codewords is atleast  $2e+1$, that is, the
    minimum distance has to be atleast  $2e+1$.
\end{proof}

\begin{example}
    If $\Cc$ is a code with minimum distance  $d=7$, then  $\Cc$ can correct up
    to  $3$ errors, since  $7=2 \cdot 3+1$. Likewise, if  $d=22$,  $\Cc$ can
    correct up to  $10$ errors.
\end{example}

\begin{remark}
    Due to this theorem, when analyzing codes, it is desirable to have codes
    with as large minumum distance as possible. Having large minimum distance
    will allow us to correct large ammounts of errors.
\end{remark}

\begin{lemma}\label{2.3.3}
    In an $(n,k)$-code $\Cc$ with minimum distance $d$ and minimum weight $w$,
    we have $d=w$.
\end{lemma}
\begin{proof}
    Recall that for any two codewords $x,y \in \Cc$,  $d_H(x,y)=w_H(y-x)$.
\end{proof}

\begin{definition}
    We call a code $\Cc$ \textbf{$e$-error-correcting} if it can correct at most
    $e$ errrs. I.e. if  $d \geq 2e+1$ for the largest possible $e$.
\end{definition}

\begin{theorem}\label{2.3.4}
    If $\Cc$ is an  $(n,k)$-code over a field $F$ with check matrix  $H$, then
    the minimum distance of  $\Cc$ is the smallest number of linearly
    independent columns of  $H$.
\end{theorem}
\begin{proof}
    For all $x \in \Cc$, we have that  $Hx^T=0$. Now, notice that  $Hx^T$ is the
    linear combination of columns $\{a_1, \dots, a_n\}$ of $H$ with the
    components of $x$. Thus, $Hx^T=x_1a_1+\dotsx_na_n=0$. Thus, if $w_H(x)=w$,
    then $Hx^T$ is linearly dependent in $w$ columns and conversely.
\end{proof}
\begin{corollary}
    If every subset of $2e$ columns of  $H$ are linearly independent, then
    $\Cc$ is $e$-error-correcting.
\end{corollary}
\begin{corollary}
    If $\Cc$ is a code over  $\F_2$, and all possible combinations less than
    $e$, of columns of  $H$ are distinct, then $d \geq 2e+1$.
\end{corollary}
