%----------------------------------------------------------------------------------------
%	SECTION 1.1
%----------------------------------------------------------------------------------------

\section{Continuous Random Variables and Vectors.}
\label{section1}

\begin{definition}
    If $X$ is a random variable with distribution  $F(x)=P(X \leq x)$, and if
    $P=\{S_i\}_{i \in \Z^+}$ is a partition of $\R$ into finite or countable
    Lebesgue measurable subsets, we define the \textbf{quantization} of $X$ by
    $P$ to be the discrete random variable $[X]_P$ with probability distribution
    \begin{equation}
        P([X]_P=i)=P(X \in \S_i)=\int_{S_i}{\dd{F(x_i)}}
    \end{equation}
\end{definition}

\begin{definition}
    We define the \textbf{mutual information} between two random variables $X$
    and  $Y$ to be
    \begin{equation}
        I(X,Y)=\sup_{P,Q}{I([X]_P,[Y]_Q)}
    \end{equation}
    where $P$ and  $Q$ are partitions of  $\R$ into finite or countable Lebesgue
    measurable subsets. If  $X=(X_1, \dots, X_n)$ and $Y=(Y_1, \dots, Y_m)$,
    then:
    \begin{equation}
        I(X,Y)=\sup{I(X,Y)}
    \end{equation}
    Where the supremum is taken over all partitions of $[X]=([X_1], \dots,
    [X_n])$ and $[Y]=([Y_1], \dots, [Y_m])$.
\end{definition}

\begin{lemma}\label{2.3.1}
    If $P_1$ and $P_2$ are partitions of $\R$, and  $P_2$ is a refinement of
    $P_1$, then $I([X]_{P_1},Y) \leq I([X]_{P_2},Y)$.
\end{lemma}
\begin{corollary}
    If $\{P_i\}_{i=1}^n$ and $\{Q_i\}_{i=1}^n$ are collections of partitions of
    $\R$, associated with the random vectors $X$ and $Y$, and if  $P_{i+1}$ is a
    refinement of $P_i$, and  $Q_{i+1}$ a refinement of $Q_i$, then:
    \begin{equation}
        I([X],[Y]) \geq I([X]_i,[Y]_i)
    \end{equation}
    For $1 \leq i \leq n$.
\end{corollary}

Now, for the following results, assume $X$ and  $Y$ have continuous joint
density  $p(x,y)$ defined by
\begin{equation*}
    P(X \in A, Y \in Y)=\int_{A}{\int_{B}{p(x,y)} \ \dd{x}\dd{y}}
\end{equation*}
for intervals $A$ and  $B$ of  $\R$ Let the densities of  $X$ and  $Y$ be
defined by
\begin{align*}
    p(x) &= \int_{-\infty}^{\infty}{p(x,y)} \ \dd{y} \\
    q(y) &= \int_{-\infty}^{\infty}{p(x,y)} \ \dd{x} \\
\end{align*}
respectively, as well as having conditional probabilities:
\begin{align*}
    p(x|y) &= \frac{p(x,y)}{q(y)} \\
    p(y|x) &= \frac{p(x,y)}{p(x)}
\end{align*}
The, assuming that
\begin{align*}
    h(X) &= \int_{-\infty}^{\infty}{p(x)\log{\inv{p(x)}}} \ \dd{x} \\
    h(X|Y) &= \int_{-\infty}^{\infty}{p(x,y)\log{\inv{p(x,y)}}} \ \dd{x} \\
\end{align*}
exist, we get:

\begin{theorem}\label{2.3.2}
    $I(X,Y)=h(X)-h(X|Y)$.
\end{theorem}
\begin{proof}
    Choose $0<\epsilon_1<\epsilon_2$ arbitrarily small and let $\{x_i\}_{i \in
    \Z}$ be a strictly increasing sequence of points such that
    $\Delta{x_i}=x_i-x_{i-1}$ satisfies $\epsilon_1<\Delta{x_i}<\epsilon_2$.
    Similarly, let $\{y_j\}_{j \in \Z}$ be a strictly increasing sequence of
    points such that $\Delta{y_j}=y_i-y_{j-1}$ satisfies
    $\epsilon_1<\Delta{y_j}<\epsilon_2$.

    Now let $[X]$ be the quantization of the random variable $X$ from the
    partiotion  $P=\{[x_{i-1},x_i) : i \in \Z\}$ of $\R$ and  let $[Y]$ be the
    quantization of the random variable $Y$ from the partiotion  $Q=\{[y_{j-1},y_j)
    : j \in \Z\}$ of $\R$. Let
    \begin{align*}
        p(i) &= P([X]=i)=\int_{x_{i-1}}^{x_i}{p(x)} \ \dd{x} \\
        q(j) &= P([Y]=j)=\int_{y_{j-1}}^{y_j}{q(y)} \ \dd{y} \\
        p(x,y) &=
        P([X]=i,[Y]=j)=\int_{y_{j-1}}^{y_j}{\int_{x_{i-1}}^{x_i}{p(x,y)}} \
        \dd{y}\dd{x} \\
    \end{align*}
    and
    \begin{equation*}
        p(i|j)=P([X]=i|[Y]=j)=\frac{p(i,j)}{1(j)}
    \end{equation*}
    Then, by the mean value theorems for integrals, we have
    $p(i)=\Delta{x_i}p(s_i)$ and  $q(j)=\Delta{y_j}p(t_j)$ for $s_i \in
    [x_{i-1}, x_i)$ and $t_j \in [y_{j-1},y_j)$. Additionally, we have
    $p(i,j)=p(s_{ij}|t_{ij})\Delta{x_i}\Delta{y_j}q(t_j)$ for $(s_{ij},t_{ij})
    \in [x_{i-1}, x_i) \times [y_{j-1},y_j)$. Thus,
    $p(i|j)=\Delta{x_i}p(s_{ij}|t_{ij})$.

    Now, we have $I([X],[Y])=H([X])-H([X]|[Y])=\sum{p(i)\log{\inv{p(i)}}}-
    \sum{p(i,j)\log{\inv{p(i,j)}}}$. Then:
    \begin{align*}
        H([X]) &= \sum{\Delta{x_i}p(s_i)\log{\inv{p(s_i)}}}+
                    \sum{\Delta{x_i}p(s_i)\log{\inv{\Delta{x_i}}}} \\
        H([X]|[Y]) &= \sum{\Delta{x_i}\Delta{y_j}p(s_{ij}|t_{ij})q(t_j)
        \log{\inv{p(s_{ij}|t_{ij})}}}+\sum{\Delta{x_i}\log{\inv{\Delta{x_i}}}
        \sum{\Delta{y_i}p(s_{ij}|t_{ij})q(y_j)}}
    \end{align*}
    Now, as $\epsilon_2 \rightarrow 0$, we see that $H([X]) \rightarrow
    h(X)+\log{\inv{\epsilon_1}}$ and $H([X]|[Y]) \rightarrow h(X|Y)+p(s_i)$.
    Thus, we see that as $\epsilon_2 \rightarrow 0$, then $I([X],[Y])
    \rightarrow h(X)-h(X|Y)$.
\end{proof}

\begin{definition}
    Assume that $X$ and  $Y$ have joint density  $p(x,y)$, with $X$ having
    density  $p(x)$. We define the \textbf{differential entropy} of $X$ to be:
    \begin{equation}
        h(X)=\int_{-\infty}^{\infty}{p(x)\log{\inv{p(x)}}} \ \dd{x}
    \end{equation}
    We define the \textbf{conditional differential entropy} of $X$ given  $Y$ to
    be:
    \begin{equation}
        h(X|Y)=\int_{-\infty}^{\infty}{p(x,y)\log{\inv{p(x,y)}}} \ \dd{x}
    \end{equation}
\end{definition}

\begin{example}
    Let $X=(X_1, \dots, X_n)$ with $X_i$ idnependent Gaussian random variables
    with mean  $\mu$ and variance  $\simga_i^2$. Then the density for  $X$ is
    \begin{equation}
        g(x)=\prod_{i=1}^n{\frac{1}{\sqrt{2\pi\sigma_i^2}}}
        \exp(-\frac{(x_i-\mu)^2}{2\sigma_i^2})
    \end{equation}
    which is called the \textbf{Gaussian density}. Now, computing the
    differential entropy of $X$ we get:
    \begin{align*}
        h(X)    &= \int{g(x)\log{\frac{1}{g(x)}}} \\
                &= \sum_{i=1}^n{\frac{1}{2}\log{2\pi\sigma_i^2}+\frac{1}{2}} \\
                &=\frac{n}{2}\log{2\pi e\sqrt[n]{\sigma_1^2 \dots \sigma_n^2}} \\
    \end{align*}
\end{example}

\begin{theorem}\label{2.3.3}
    IF $X=(X_1, \dots, X_n)$ has density $p(x)$, and if
    $E((x_i-\mu_i)^2)=\sigma_i^2$ for $1 \leq i \leq n$, then $h(X) \leq
    \frac{n}{2}\log{2\pi e\sqrt[n]{\sigma_1^2 \dots \sigma_n^2}}$; with equality
    holding if, and only if $p(x)=g(x)$.
\end{theorem}
\begin{proof}
    By hypothesis, if $p_i(x)$ is the marginal distribution of $X_i$, we have
    $\int{p_i(x)} \ \dd{x}=1$ and $\int{p_i(x)(x_i-\mu_i)^2} \
    \dd{x}=\sigma_i^2$. Thus
    \begin{equation*}
        \int{p(x)\log{\frac{1}{g(x)}}} \ \dd{x}=
        \frac{n}{2}\log{2\pi e\sqrt[n]{\sigma_1^2 \dots \sigma_n^2}}
    \end{equation*}
    So if $Y$ is an  $n$-dimensional random Gaussian  (i.e. it has Gaussian
    distribution) vector, then
    \begin{equation*}
        h(X)-h(X|Y)=\int{p(x)\log{\frac{g(x)}{p(x)}}} \ \dd{x} \leq
        \log{\int{g(x)} \ \dd{x}}=0
    \end{equation*}
    by Jensen's inequality.
\end{proof}

We now state, but don't prove the theorems from previous sections which are true
for continuous random variables.

\begin{theorem}\label{2.3.4}
    $I(X,Y) \geq 0$.
\end{theorem}

\begin{theorem}\label{2.3.5}
    $I([X],[Y]) \geq \sum{I([X_i],[Y_i])}$.
\end{theorem}

\begin{theorem}\label{2.3.6}
    $p(y|x)=\prod{p(x_i|y_i)}$.
\end{theorem}
\begin{corollary}
    For any discrete memoryless channel,
    \begin{equation*}
        I([X],[Y]) \leq \sum{I([X_i], [Y_i])}
    \end{equation*}
\end{corollary}
